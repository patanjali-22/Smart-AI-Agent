# ðŸ¤– Smart AI Agent â€” RAG + Web Search powered by LangGraph

An intelligent AI agent that answers questions by combining **Retrieval-Augmented Generation (RAG)** with **real-time web search**. Built with a modular architecture using FastAPI, Streamlit, and LangGraph â€” giving users full control and transparency over how answers are generated.

```mermaid
graph TD
    User["ðŸ‘¤ User"] --> Frontend

    subgraph Frontend ["Streamlit Frontend"]
        A["Chat UI"]
        B["PDF Upload"]
        C["Web Search Toggle"]
        D["Agent Trace Viewer"]
    end

    Frontend -->|"HTTP POST /chat/"| Backend
    Frontend -->|"HTTP POST /upload-document/"| Backend

    subgraph Backend ["FastAPI Backend"]
        E["REST API Endpoints"]
        F["Request Handling"]
    end

    Backend --> Agent

    subgraph Agent ["LangGraph Agent Core"]
        Router["Router Node"] -->|"route = rag"| RAG["RAG Lookup Node"]
        Router -->|"route = web"| Web["Web Search Node"]
        Router -->|"route = answer"| Answer["Answer Node"]
        RAG -->|"sufficient"| Answer
        RAG -->|"not sufficient"| Web
        Web --> Answer
    end

    RAG --> Pinecone["ðŸŸ¦ Pinecone - Vector DB"]
    RAG --> HF["ðŸŸ¡ HuggingFace - Embeddings"]
    Web --> Tavily["ðŸŸ¢ Tavily - Web Search API"]
    Answer --> Groq["ðŸŸ£ Groq - Llama 3 70B"]
    Router --> Groq
```

---

## âœ¨ Features

- **Intelligent Query Routing** â€” Automatically decides whether to search internal documents, the web, or answer directly
- **RAG with Pinecone** â€” Upload PDFs and query them instantly using vector similarity search
- **Live Web Search** â€” Falls back to Tavily web search when internal knowledge is insufficient
- **Sufficiency Judgment** â€” An LLM judge evaluates if RAG results actually answer the question before responding
- **User-Controlled Web Toggle** â€” Enable or disable web search from the UI at any time
- **Agent Trace Viewer** â€” See exactly how the agent routed your query, what it retrieved, and why
- **Conversation Memory** â€” LangGraph checkpointing maintains context across multiple turns
- **PDF Upload** â€” Drag-and-drop PDF ingestion into the knowledge base

---

## ðŸ—ï¸ Architecture

The application is split into three layers:

| Layer | Technology | Role |
|-------|-----------|------|
| **Frontend** | Streamlit | Chat UI, file upload, settings, trace viewer |
| **Backend** | FastAPI | REST API, request handling, document processing |
| **Agent Core** | LangGraph | Query routing, RAG retrieval, web search, answer generation |

### Agent Workflow

```
User Query â†’ Router â†’ RAG Lookup â†’ Sufficiency Check â†’ Answer
                 â†“                        â†“ (insufficient)
            Web Search â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
              Answer
```

The **Router** analyzes each query and picks the best path. If RAG retrieval is insufficient, a **Judge LLM** detects this and reroutes to web search automatically.

---

## ðŸ§© Project Structure

```
Smart-AI-Agent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ config.py              # Environment variables and API keys
â”‚   â”œâ”€â”€ vectorstore.py         # Pinecone vector store + HuggingFace embeddings
â”‚   â”œâ”€â”€ agent.py               # LangGraph agent with 4-node state machine
â”‚   â””â”€â”€ main.py                # FastAPI server with chat and upload endpoints
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ config.py              # Frontend configuration
â”‚   â”œâ”€â”€ session_manager.py     # Streamlit session state management
â”‚   â”œâ”€â”€ backend_api.py         # HTTP client for backend communication
â”‚   â”œâ”€â”€ ui_components.py       # Reusable UI components
â”‚   â””â”€â”€ app.py                 # Streamlit application entry point
â”‚
â”œâ”€â”€ data/                      # Sample documents for RAG
â”œâ”€â”€ assets/                    # Architecture diagrams and images
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ .env                       # API keys (not committed)
```

---

## âš™ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| Language | Python 3.10+ |
| Frontend | Streamlit |
| Backend | FastAPI + Uvicorn |
| Agent Framework | LangGraph + LangChain |
| LLM | Groq (Llama 3 70B) |
| Embeddings | HuggingFace `all-MiniLM-L6-v2` (384 dims) |
| Vector Store | Pinecone (Serverless) |
| Web Search | Tavily Search API |
| PDF Processing | PyPDFLoader |

---

## ðŸš€ Getting Started

### Prerequisites

- Python 3.10+
- API keys for **Groq**, **Pinecone**, and **Tavily**

### Installation

```bash
git clone https://github.com/patanjali-22/Smart-AI-Agent.git
cd Smart-AI-Agent
python -m venv .venv
.venv\Scripts\activate       # Windows
pip install -r requirements.txt
```

### Environment Variables

Create a `.env` file in the project root:

```env
GROQ_API_KEY=your_groq_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=us-east-1
TAVILY_API_KEY=your_tavily_api_key
FASTAPI_BASE_URL=http://localhost:8000
```

### Run

```bash
# Terminal 1 â€” Start the backend
cd backend
uvicorn main:app --reload --port 8000

# Terminal 2 â€” Start the frontend
streamlit run frontend/app.py
```

---

## ðŸ“¡ API Endpoints

### `POST /upload-document/`

Upload a PDF to the RAG knowledge base.

```json
// Request: multipart/form-data with file field
// Response:
{
  "message": "PDF 'document.pdf' successfully uploaded and indexed.",
  "filename": "document.pdf",
  "processed_chunks": 12
}
```

### `POST /chat/`

Send a query to the agent.

```json
// Request:
{
  "session_id": "abc-123",
  "query": "What are the symptoms of diabetes?",
  "enable_web_search": true
}

// Response:
{
  "response": "The common symptoms of diabetes include...",
  "trace_events": [
    { "step": 1, "node_name": "router", "description": "Router decided: 'rag'", "event_type": "router_decision" },
    { "step": 2, "node_name": "rag_lookup", "description": "RAG Lookup performed. Content found and deemed sufficient.", "event_type": "rag_action" }
  ]
}
```

### `GET /health`

Health check endpoint.

---

## â˜ï¸ Cloud Deployment

This project is deployment-ready with configs for **Render** (backend) and **Streamlit Cloud** (frontend).

### Backend â†’ Render (Free Tier)

1. Go to [render.com](https://render.com) and connect your GitHub repo
2. Create a **New Web Service** â†’ select this repository
3. Render will auto-detect the `render.yaml` blueprint, or configure manually:
   - **Build Command**: `pip install -r requirements.txt`
   - **Start Command**: `cd backend && uvicorn main:app --host 0.0.0.0 --port $PORT`
4. Add your environment variables in the Render dashboard:
   - `GROQ_API_KEY`
   - `PINECONE_API_KEY`
   - `PINECONE_ENVIRONMENT`
   - `TAVILY_API_KEY`
5. Deploy â€” your backend will be live at `https://your-app.onrender.com`

### Frontend â†’ Streamlit Cloud (Free)

1. Go to [share.streamlit.io](https://share.streamlit.io)
2. Connect your GitHub repo and select `frontend/app.py` as the main file
3. Add secrets in the Streamlit Cloud dashboard:
   - `FASTAPI_BASE_URL = https://your-app.onrender.com` (your Render backend URL)
4. Deploy â€” your frontend will be live at `https://your-app.streamlit.app`

### Using Docker

```bash
# Build the image
docker build -t smart-ai-agent .

# Run the container
docker run -p 8000:8000 --env-file .env smart-ai-agent
```

---

## ðŸ”® Future Improvements

- Token-by-token streaming for real-time responses
- Advanced RAG techniques (reranking, multi-query retrieval)
- User authentication and conversation history persistence
- Additional tools: calculator, code interpreter, calendar
- Dark mode and enhanced UI animations

---

## ðŸ“„ License

This project is open source and available for learning and personal use.

---

> Built with LangGraph, LangChain, Groq, Pinecone, and Streamlit
